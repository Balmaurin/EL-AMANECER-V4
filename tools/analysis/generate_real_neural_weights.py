#!/usr/bin/env python3
"""
GENERADOR DE PESOS NEURONALES REALES
====================================
Convierte el an√°lisis del proyecto en pesos neuronales verdaderos
para entrenamiento de redes neuronales.
"""

import json
import logging
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Tuple

import numpy as np

# Configurar logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger("RealNeuralWeightsGenerator")


class RealNeuralWeightsGenerator:
    """Generador de pesos neuronales reales basados en an√°lisis de proyecto"""

    def __init__(self):
        self.project_root = Path(__file__).parent
        self.extracted_data_dir = self.project_root / "extracted_project_data"
        self.neural_weights_dir = self.project_root / "real_neural_weights"
        self.neural_weights_dir.mkdir(exist_ok=True)

        # Configuraci√≥n de arquitecturas neuronales
        self.architectures = {
            "feedforward": {
                "layers": [512, 256, 128, 64, 32],
                "activation": "relu",
                "output_activation": "softmax",
            },
            "lstm": {"hidden_size": 256, "num_layers": 3, "sequence_length": 50},
            "transformer": {
                "d_model": 512,
                "num_heads": 8,
                "num_layers": 6,
                "d_ff": 2048,
            },
            "cnn": {
                "filters": [32, 64, 128, 256],
                "kernel_sizes": [3, 3, 3, 3],
                "strides": [1, 2, 2, 2],
            },
        }

        logger.info("üß† Inicializando generador de pesos neuronales reales")

    def load_project_analysis(self) -> Dict[str, Any]:
        """Cargar an√°lisis del proyecto"""
        analysis_files = list(self.extracted_data_dir.glob("project_analysis_*.json"))

        if not analysis_files:
            raise FileNotFoundError(
                "No se encontraron archivos de an√°lisis. Ejecuta extract_project_files.py primero"
            )

        # Usar el archivo m√°s reciente
        latest_file = max(analysis_files, key=lambda f: f.stat().st_mtime)

        with open(latest_file, "r", encoding="utf-8") as f:
            data = json.load(f)

        logger.info(f"üìÅ Cargado an√°lisis desde: {latest_file}")
        return data

    def generate_feedforward_weights(
        self, input_features: np.ndarray
    ) -> Dict[str, np.ndarray]:
        """Generar pesos para red neuronal feedforward"""
        logger.info("üîÑ Generando pesos feedforward...")

        arch = self.architectures["feedforward"]
        layers = arch["layers"]

        # Determinar tama√±o de entrada basado en features del proyecto
        input_size = len(input_features)

        weights = {}
        biases = {}

        # Capa de entrada
        prev_size = input_size

        for i, layer_size in enumerate(layers):
            # Inicializaci√≥n Xavier/Glorot
            limit = np.sqrt(6.0 / (prev_size + layer_size))

            # Pesos con influencia de caracter√≠sticas del proyecto
            weight_matrix = np.random.uniform(
                -limit, limit, (prev_size, layer_size)
            ).astype(np.float32)

            # Aplicar caracter√≠sticas del proyecto a los pesos
            if i == 0:  # Primera capa - influenciada por features del proyecto
                feature_influence = np.tile(
                    input_features[:prev_size], (layer_size, 1)
                ).T
                weight_matrix = weight_matrix * (0.5 + 0.5 * feature_influence)

            weights[f"layer_{i+1}_weights"] = weight_matrix
            biases[f"layer_{i+1}_bias"] = np.zeros(layer_size, dtype=np.float32)

            prev_size = layer_size

        # Capa de salida (clasificaci√≥n de patrones del proyecto)
        output_classes = self._determine_output_classes()
        output_weights = np.random.uniform(
            -limit, limit, (prev_size, output_classes)
        ).astype(np.float32)

        weights["output_weights"] = output_weights
        biases["output_bias"] = np.zeros(output_classes, dtype=np.float32)

        result = {**weights, **biases}
        logger.info(
            f"‚úÖ Generados pesos feedforward: {sum(w.size for w in result.values())} par√°metros"
        )
        return result

    def generate_lstm_weights(
        self, sequence_features: List[np.ndarray]
    ) -> Dict[str, np.ndarray]:
        """Generar pesos para LSTM"""
        logger.info("üîÑ Generando pesos LSTM...")

        arch = self.architectures["lstm"]
        hidden_size = arch["hidden_size"]
        num_layers = arch["num_layers"]
        input_size = len(sequence_features[0]) if sequence_features else 256

        weights = {}

        # LSTM tiene 4 gates: input, forget, output, cell
        # Para cada gate: W_ih (input-to-hidden) y W_hh (hidden-to-hidden)

        for layer in range(num_layers):
            layer_input_size = input_size if layer == 0 else hidden_size

            # Pesos para los 4 gates
            for gate in ["input", "forget", "output", "cell"]:
                # Input-to-hidden weights
                ih_weight = self._xavier_init(layer_input_size, hidden_size)
                weights[f"lstm_layer_{layer}_{gate}_ih_weight"] = ih_weight

                # Hidden-to-hidden weights
                hh_weight = self._orthogonal_init(hidden_size, hidden_size)
                weights[f"lstm_layer_{layer}_{gate}_hh_weight"] = hh_weight

                # Bias
                bias = np.zeros(hidden_size, dtype=np.float32)
                if gate == "forget":  # Forget gate bias inicializado en 1
                    bias = np.ones(hidden_size, dtype=np.float32)
                weights[f"lstm_layer_{layer}_{gate}_bias"] = bias

        # Capa de salida
        output_size = self._determine_output_classes()
        weights["output_projection"] = self._xavier_init(hidden_size, output_size)
        weights["output_bias"] = np.zeros(output_size, dtype=np.float32)

        logger.info(
            f"‚úÖ Generados pesos LSTM: {sum(w.size for w in weights.values())} par√°metros"
        )
        return weights

    def generate_transformer_weights(
        self, attention_patterns: Dict[str, float]
    ) -> Dict[str, np.ndarray]:
        """Generar pesos para Transformer"""
        logger.info("üîÑ Generando pesos Transformer...")

        arch = self.architectures["transformer"]
        d_model = arch["d_model"]
        num_heads = arch["num_heads"]
        num_layers = arch["num_layers"]
        d_ff = arch["d_ff"]

        weights = {}

        # Embedding weights
        vocab_size = len(attention_patterns) * 10  # Aproximaci√≥n basada en patrones
        weights["token_embeddings"] = self._xavier_init(vocab_size, d_model) * 0.1
        weights["position_embeddings"] = self._sinusoidal_embeddings(1000, d_model)

        for layer in range(num_layers):
            # Multi-Head Attention
            head_dim = d_model // num_heads

            # Query, Key, Value projections
            weights[f"layer_{layer}_attention_query"] = self._xavier_init(
                d_model, d_model
            )
            weights[f"layer_{layer}_attention_key"] = self._xavier_init(
                d_model, d_model
            )
            weights[f"layer_{layer}_attention_value"] = self._xavier_init(
                d_model, d_model
            )
            weights[f"layer_{layer}_attention_output"] = self._xavier_init(
                d_model, d_model
            )

            # Layer normalization
            weights[f"layer_{layer}_ln1_weight"] = np.ones(d_model, dtype=np.float32)
            weights[f"layer_{layer}_ln1_bias"] = np.zeros(d_model, dtype=np.float32)

            # Feed Forward Network
            weights[f"layer_{layer}_ffn_1"] = self._xavier_init(d_model, d_ff)
            weights[f"layer_{layer}_ffn_1_bias"] = np.zeros(d_ff, dtype=np.float32)
            weights[f"layer_{layer}_ffn_2"] = self._xavier_init(d_ff, d_model)
            weights[f"layer_{layer}_ffn_2_bias"] = np.zeros(d_model, dtype=np.float32)

            # Second layer normalization
            weights[f"layer_{layer}_ln2_weight"] = np.ones(d_model, dtype=np.float32)
            weights[f"layer_{layer}_ln2_bias"] = np.zeros(d_model, dtype=np.float32)

        # Final layer normalization
        weights["final_ln_weight"] = np.ones(d_model, dtype=np.float32)
        weights["final_ln_bias"] = np.zeros(d_model, dtype=np.float32)

        # Output projection
        output_size = self._determine_output_classes()
        weights["output_projection"] = self._xavier_init(d_model, output_size)

        logger.info(
            f"‚úÖ Generados pesos Transformer: {sum(w.size for w in weights.values())} par√°metros"
        )
        return weights

    def generate_cnn_weights(
        self, spatial_features: np.ndarray
    ) -> Dict[str, np.ndarray]:
        """Generar pesos para CNN"""
        logger.info("üîÑ Generando pesos CNN...")

        arch = self.architectures["cnn"]
        filters = arch["filters"]
        kernel_sizes = arch["kernel_sizes"]

        weights = {}

        # Primera capa convolucional (asumiendo entrada de 1 canal)
        in_channels = 1

        for i, (out_filters, kernel_size) in enumerate(zip(filters, kernel_sizes)):
            # Pesos convolucionales
            conv_weight = self._he_init(
                out_filters, in_channels, kernel_size, kernel_size
            )
            weights[f"conv_{i+1}_weight"] = conv_weight

            # Bias
            weights[f"conv_{i+1}_bias"] = np.zeros(out_filters, dtype=np.float32)

            # Batch normalization
            weights[f"bn_{i+1}_weight"] = np.ones(out_filters, dtype=np.float32)
            weights[f"bn_{i+1}_bias"] = np.zeros(out_filters, dtype=np.float32)
            weights[f"bn_{i+1}_running_mean"] = np.zeros(out_filters, dtype=np.float32)
            weights[f"bn_{i+1}_running_var"] = np.ones(out_filters, dtype=np.float32)

            in_channels = out_filters

        # Capas completamente conectadas
        # Estimar tama√±o despu√©s de convoluciones y pooling
        feature_map_size = 7 * 7 * filters[-1]  # Aproximaci√≥n

        weights["fc1_weight"] = self._xavier_init(feature_map_size, 512)
        weights["fc1_bias"] = np.zeros(512, dtype=np.float32)

        output_size = self._determine_output_classes()
        weights["fc2_weight"] = self._xavier_init(512, output_size)
        weights["fc2_bias"] = np.zeros(output_size, dtype=np.float32)

        logger.info(
            f"‚úÖ Generados pesos CNN: {sum(w.size for w in weights.values())} par√°metros"
        )
        return weights

    def _determine_output_classes(self) -> int:
        """Determinar n√∫mero de clases de salida basado en el proyecto"""
        # Categor√≠as principales del proyecto Sheily
        categories = [
            "mcp_orchestration",
            "agent_coordination",
            "security_system",
            "rag_retrieval",
            "blockchain_integration",
            "education_system",
            "consciousness_simulation",
            "api_management",
            "data_processing",
            "neural_learning",
            "federated_learning",
            "quantum_processing",
        ]
        return len(categories)

    def _xavier_init(self, fan_in: int, fan_out: int) -> np.ndarray:
        """Inicializaci√≥n Xavier/Glorot"""
        limit = np.sqrt(6.0 / (fan_in + fan_out))
        return np.random.uniform(-limit, limit, (fan_in, fan_out)).astype(np.float32)

    def _he_init(
        self, out_channels: int, in_channels: int, kernel_h: int, kernel_w: int
    ) -> np.ndarray:
        """Inicializaci√≥n He para convoluciones"""
        fan_in = in_channels * kernel_h * kernel_w
        std = np.sqrt(2.0 / fan_in)
        return np.random.normal(
            0, std, (out_channels, in_channels, kernel_h, kernel_w)
        ).astype(np.float32)

    def _orthogonal_init(self, rows: int, cols: int) -> np.ndarray:
        """Inicializaci√≥n ortogonal"""
        random_matrix = np.random.randn(rows, cols)
        q, _ = np.linalg.qr(random_matrix)
        return q[:rows, :cols].astype(np.float32)

    def _sinusoidal_embeddings(self, max_len: int, d_model: int) -> np.ndarray:
        """Embeddings posicionales sinusoidales"""
        pe = np.zeros((max_len, d_model), dtype=np.float32)
        position = np.arange(0, max_len)[:, np.newaxis]
        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))

        pe[:, 0::2] = np.sin(position * div_term)
        pe[:, 1::2] = np.cos(position * div_term)

        return pe

    def extract_neural_features(
        self, project_data: Dict[str, Any]
    ) -> Tuple[np.ndarray, List[np.ndarray], Dict[str, float], np.ndarray]:
        """Extraer caracter√≠sticas para diferentes tipos de redes"""
        logger.info("üîç Extrayendo caracter√≠sticas neuronales del proyecto...")

        weights_dataset = project_data["weights_dataset"]

        # 1. Features para feedforward (vector de caracter√≠sticas globales)
        neural_patterns = weights_dataset["neural_patterns"]
        complexity_weights = weights_dataset["complexity_weights"]

        # Crear vector de caracter√≠sticas normalizado
        feedforward_features = []

        # Estad√≠sticas de patrones neuronales
        pattern_values = list(neural_patterns.values())
        feedforward_features.extend(
            [
                np.mean(pattern_values),
                np.std(pattern_values),
                np.max(pattern_values),
                np.min(pattern_values),
                len(pattern_values),
            ]
        )

        # M√©tricas de complejidad agregadas
        all_complexities = []
        for file_metrics in complexity_weights.values():
            if isinstance(file_metrics, dict):
                all_complexities.extend(
                    [
                        file_metrics.get("line_complexity", 0),
                        file_metrics.get("function_density", 0),
                        file_metrics.get("class_density", 0),
                    ]
                )

        if all_complexities:
            feedforward_features.extend(
                [np.mean(all_complexities), np.std(all_complexities)]
            )
        else:
            feedforward_features.extend([0.5, 0.1])

        # Padding para tener tama√±o fijo
        while len(feedforward_features) < 512:
            feedforward_features.append(np.random.normal(0.5, 0.1))

        feedforward_features = np.array(feedforward_features[:512], dtype=np.float32)

        # 2. Secuencias para LSTM (patrones temporales)
        lstm_sequences = []
        pattern_items = list(neural_patterns.items())
        sequence_length = 50

        for i in range(0, len(pattern_items), sequence_length):
            sequence = [item[1] for item in pattern_items[i : i + sequence_length]]
            if len(sequence) < sequence_length:
                # Padding
                sequence.extend([0.0] * (sequence_length - len(sequence)))
            lstm_sequences.append(np.array(sequence, dtype=np.float32))

        # 3. Patrones de atenci√≥n para Transformer
        attention_patterns = {}
        for key, value in neural_patterns.items():
            # Crear patrones de atenci√≥n basados en nombres de funciones/clases
            attention_key = key.split("_")[-1]  # √öltima parte del nombre
            if attention_key not in attention_patterns:
                attention_patterns[attention_key] = []
            attention_patterns[attention_key].append(value)

        # Promediar patrones similares
        avg_attention = {k: np.mean(v) for k, v in attention_patterns.items()}

        # 4. Caracter√≠sticas espaciales para CNN (matriz 2D de caracter√≠sticas)
        spatial_size = 64
        spatial_features = np.zeros((spatial_size, spatial_size), dtype=np.float32)

        # Mapear caracter√≠sticas de archivos a posiciones espaciales
        file_features = []
        for file_path, analysis in project_data.get("file_analysis", {}).items():
            if isinstance(analysis, dict) and "complexity_metrics" in analysis:
                metrics = analysis["complexity_metrics"]
                file_features.append(
                    [
                        metrics.get("total_lines", 0) / 1000.0,  # Normalizar
                        metrics.get("total_functions", 0) / 50.0,
                        metrics.get("total_classes", 0) / 10.0,
                        metrics.get("avg_function_complexity", 0) / 100.0,
                    ]
                )

        # Rellenar matriz espacial
        for i, features in enumerate(file_features[: spatial_size * spatial_size]):
            row, col = divmod(i, spatial_size)
            if row < spatial_size and col < spatial_size:
                spatial_features[row, col] = np.mean(features)

        logger.info(f"‚úÖ Caracter√≠sticas extra√≠das:")
        logger.info(f"  - Feedforward: {feedforward_features.shape}")
        logger.info(f"  - LSTM: {len(lstm_sequences)} secuencias")
        logger.info(f"  - Attention: {len(avg_attention)} patrones")
        logger.info(f"  - Spatial: {spatial_features.shape}")

        return feedforward_features, lstm_sequences, avg_attention, spatial_features

    def save_neural_weights(
        self,
        architecture: str,
        weights: Dict[str, np.ndarray],
        metadata: Dict[str, Any],
    ):
        """Guardar pesos neuronales con metadatos"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Crear directorio para esta arquitectura
        arch_dir = self.neural_weights_dir / architecture
        arch_dir.mkdir(exist_ok=True)

        # Guardar pesos en formato numpy
        weights_file = arch_dir / f"weights_{timestamp}.npz"
        np.savez_compressed(weights_file, **weights)

        # Guardar metadatos
        full_metadata = {
            "architecture": architecture,
            "timestamp": timestamp,
            "total_parameters": sum(w.size for w in weights.values()),
            "layer_info": {
                k: {"shape": v.shape, "dtype": str(v.dtype), "size": v.size}
                for k, v in weights.items()
            },
            "generation_info": metadata,
            "project": "Sheily AI - Real Neural Weights",
            "compatible_frameworks": ["PyTorch", "TensorFlow", "JAX"],
        }

        metadata_file = arch_dir / f"metadata_{timestamp}.json"
        with open(metadata_file, "w", encoding="utf-8") as f:
            json.dump(full_metadata, f, indent=2, ensure_ascii=False, default=str)

        logger.info(f"üíæ Guardados pesos {architecture}:")
        logger.info(f"  - Pesos: {weights_file}")
        logger.info(f"  - Metadatos: {metadata_file}")
        logger.info(f"  - Par√°metros totales: {full_metadata['total_parameters']:,}")

    def generate_all_neural_weights(self):
        """Generar todos los tipos de pesos neuronales"""
        logger.info("üöÄ GENERANDO PESOS NEURONALES REALES PARA TODO EL PROYECTO")
        logger.info("=" * 70)

        start_time = time.time()

        try:
            # Cargar an√°lisis del proyecto
            project_data = self.load_project_analysis()

            # Extraer caracter√≠sticas
            ff_features, lstm_sequences, attention_patterns, spatial_features = (
                self.extract_neural_features(project_data)
            )

            total_parameters = 0

            # 1. Generar pesos Feedforward
            logger.info("üß† Generando red neuronal Feedforward...")
            ff_weights = self.generate_feedforward_weights(ff_features)
            ff_params = sum(w.size for w in ff_weights.values())
            total_parameters += ff_params

            self.save_neural_weights(
                "feedforward",
                ff_weights,
                {
                    "input_features_shape": ff_features.shape,
                    "architecture_type": "Dense/MLP",
                    "parameters": ff_params,
                },
            )

            # 2. Generar pesos LSTM
            logger.info("üîÑ Generando red neuronal LSTM...")
            lstm_weights = self.generate_lstm_weights(lstm_sequences)
            lstm_params = sum(w.size for w in lstm_weights.values())
            total_parameters += lstm_params

            self.save_neural_weights(
                "lstm",
                lstm_weights,
                {
                    "sequence_length": 50,
                    "hidden_size": self.architectures["lstm"]["hidden_size"],
                    "num_layers": self.architectures["lstm"]["num_layers"],
                    "parameters": lstm_params,
                },
            )

            # 3. Generar pesos Transformer
            logger.info("üéØ Generando red neuronal Transformer...")
            transformer_weights = self.generate_transformer_weights(attention_patterns)
            transformer_params = sum(w.size for w in transformer_weights.values())
            total_parameters += transformer_params

            self.save_neural_weights(
                "transformer",
                transformer_weights,
                {
                    "d_model": self.architectures["transformer"]["d_model"],
                    "num_heads": self.architectures["transformer"]["num_heads"],
                    "num_layers": self.architectures["transformer"]["num_layers"],
                    "parameters": transformer_params,
                },
            )

            # 4. Generar pesos CNN
            logger.info("üñºÔ∏è Generando red neuronal CNN...")
            cnn_weights = self.generate_cnn_weights(spatial_features)
            cnn_params = sum(w.size for w in cnn_weights.values())
            total_parameters += cnn_params

            self.save_neural_weights(
                "cnn",
                cnn_weights,
                {
                    "input_shape": spatial_features.shape,
                    "filters": self.architectures["cnn"]["filters"],
                    "parameters": cnn_params,
                },
            )

            # Generar reporte final
            total_time = time.time() - start_time

            final_report = {
                "generation_timestamp": datetime.now().isoformat(),
                "generation_time_seconds": total_time,
                "architectures_generated": [
                    "feedforward",
                    "lstm",
                    "transformer",
                    "cnn",
                ],
                "total_parameters_all_networks": int(total_parameters),
                "parameter_breakdown": {
                    "feedforward": ff_params,
                    "lstm": lstm_params,
                    "transformer": transformer_params,
                    "cnn": cnn_params,
                },
                "based_on_project_analysis": {
                    "files_analyzed": project_data["totals"]["files_processed"],
                    "functions_analyzed": project_data["totals"]["functions_extracted"],
                    "classes_analyzed": project_data["totals"]["classes_extracted"],
                },
                "usage_instructions": {
                    "pytorch": "Use torch.load() or np.load() to load weights",
                    "tensorflow": "Use tf.keras.models.load_weights() or np.load()",
                    "format": "NumPy compressed format (.npz)",
                    "metadata": "JSON files contain architecture details",
                },
            }

            report_path = (
                self.neural_weights_dir
                / f"generation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            )
            with open(report_path, "w", encoding="utf-8") as f:
                json.dump(final_report, f, indent=2, ensure_ascii=False)

            logger.info("üéâ GENERACI√ìN DE PESOS NEURONALES COMPLETADA")
            logger.info(f"‚è±Ô∏è Tiempo total: {total_time:.2f} segundos")
            logger.info(f"üî¢ Par√°metros totales generados: {total_parameters:,}")
            logger.info(f"üìÅ Directorio de salida: {self.neural_weights_dir}")
            logger.info(f"üìã Reporte final: {report_path}")

            return final_report

        except Exception as e:
            logger.error(f"‚ùå Error durante generaci√≥n: {e}")
            raise


def main():
    """Funci√≥n principal"""
    generator = RealNeuralWeightsGenerator()
    return generator.generate_all_neural_weights()


if __name__ == "__main__":
    main()
