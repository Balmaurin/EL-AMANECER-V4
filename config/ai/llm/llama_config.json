{
  "model_path": "modelsLLM/llama-3.2-3b/Llama-3.2-3B-Instruct-f16.gguf",
  "model_type": "llama",
  "context_size": 8192,
  "threads": 4,
  "gpu_layers": 0,
  "temperature": 0.7,
  "top_p": 0.9,
  "top_k": 40,
  "max_tokens": 512,
  "repeat_penalty": 1.1,
  "repeat_last_n": 64,
  "seed": -1
}