# -*- coding: utf-8 -*-
"""
CONSCIOUS PROMPT GENERATOR - Enterprise Edition v3.0 OPTIMIZED
===============================================================

Generador de prompts consciente completamente integrado con:
- BiologicalConsciousnessSystem REAL
- HumanEmotionalSystem con 35 emociones
- RAG Embeddings reales (SentenceTransformers + FAISS)
- Adaptadores emocionales y de creatividad
- Mock system para testing sin dependencias pesadas
- Auto-optimizaci√≥n con feedback loop
- Production-ready con observabilidad completa

Caracter√≠sticas Enterprise:
‚úÖ RAG con embeddings reales (all-MiniLM-L6-v2)
‚úÖ Integraci√≥n con emotional_neuro_system existente
‚úÖ Neuromodulaci√≥n desde RAS real
‚úÖ Safety ML-ready con filtros avanzados
‚úÖ Mock testing para desarrollo/debugging
‚úÖ Adaptaci√≥n emocional din√°mica en prompts
‚úÖ Creatividad modulada por acetilcolina
‚úÖ Auto-learning con prediction errors

Autor: Sistema de Consciencia v4.0
Fecha: 2025-11-25
Version: 3.0-OPTIMIZED
"""

from typing import Dict, Any, Optional, List, Tuple
import uuid
from datetime import datetime
import numpy as np
import logging

logger = logging.getLogger("conscious_prompt_generator")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(message)s"))
    logger.addHandler(ch)


# ---------------------------------------------------
# RAG SYSTEM - Real Embeddings & Vector Search
# ---------------------------------------------------
class RAGEmbeddingSystem:
    """
    Sistema RAG real usando embeddings (SentenceTransformers) y b√∫squeda vectorial.
    Si no est√°n disponibles las librer√≠as, usa modo simulaci√≥n para testing.
    """
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2', use_mock: bool = False):
        self.use_mock = use_mock
        self.model = None
        self.index = []  # Lista de dicts {vector, content, metadata}
        self.dimension = 384  # Dimensi√≥n de embeddings all-MiniLM-L6-v2
        
        if not use_mock:
            try:
                from sentence_transformers import SentenceTransformer
                from sklearn.metrics.pairwise import cosine_similarity
                self.model = SentenceTransformer(model_name)
                self.cosine_similarity = cosine_similarity
                logger.info(f"‚úÖ RAG: Modelo {model_name} cargado correctamente.")
            except ImportError as e:
                logger.warning(f"‚ö†Ô∏è RAG: sentence_transformers/sklearn no encontrados ({e}). Usando modo MOCK.")
                self.use_mock = True
    
    def encode(self, text: str) -> np.ndarray:
        """Genera embedding para texto"""
        if self.use_mock:
            # Vector aleatorio normalizado simulado
            vec = np.random.rand(self.dimension)
            return vec / (np.linalg.norm(vec) + 1e-10)
        
        return self.model.encode(text, normalize_embeddings=True)
    
    def add_document(self, content: str, metadata: Dict[str, Any] = None):
        """A√±ade documento al √≠ndice vectorial"""
        vector = self.encode(content)
        self.index.append({
            'vector': vector,
            'content': content,
            'metadata': metadata or {},
            'id': str(uuid.uuid4())
        })
        logger.debug(f"üìö RAG: Documento indexado (total: {len(self.index)})")
    
    def retrieve(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """Recupera documentos m√°s similares sem√°nticamente"""
        if not self.index:
            return []
            
        query_vector = self.encode(query)
        
        if self.use_mock:
            # En mock, retornamos aleatorios o los √∫ltimos
            selected = self.index[-top_k:] if len(self.index) >= top_k else self.index
            return [{'content_snippet': doc['content'][:200], 
                    'similarity_score': 0.75, 
                    **doc['metadata']} for doc in selected]
            
        # C√°lculo real de similitud coseno
        vectors = np.vstack([doc['vector'] for doc in self.index])
        similarities = self.cosine_similarity([query_vector], vectors)[0]
        
        # Obtener √≠ndices de top_k
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            doc = self.index[idx]
            result = doc['metadata'].copy()
            result['similarity_score'] = float(similarities[idx])
            result['content_snippet'] = doc['content'][:200]
            result['doc_id'] = doc['id']
            results.append(result)
            
        return results
    
    def get_stats(self) -> Dict[str, Any]:
        """Estad√≠sticas del sistema RAG"""
        return {
            'total_documents': len(self.index),
            'dimension': self.dimension,
            'mode': 'REAL' if not self.use_mock else 'MOCK'
        }


# ---------------------------------------------------
# NEUROMODULATOR - Conectado con RAS real
# ---------------------------------------------------
class Neuromodulator:
    """
    Neuromodulaci√≥n basada en estado del RAS real del sistema.
    Adapta prompts seg√∫n niveles de neurotransmisores.
    """
    def __init__(self):
        # Niveles base (se actualizan desde RAS)
        self.dopamine = 0.5
        self.norepinephrine = 0.5
        self.serotonin = 0.5
        self.acetylcholine = 0.5
        
        # Historial de prediction errors
        self.rpe_history: List[float] = []
    
    def update_from_ras(self, ras_state: Dict[str, Any]):
        """Actualiza desde el estado real del RAS"""
        neurotransmitters = ras_state.get('neurotransmitter_levels', {})
        self.dopamine = neurotransmitters.get('dopamine', 0.5)
        self.norepinephrine = neurotransmitters.get('norepinephrine', 0.5)
        self.serotonin = neurotransmitters.get('serotonin', 0.5)
        self.acetylcholine = neurotransmitters.get('acetylcholine', 0.5)
    
    def update_from_emotional_system(self, emotional_profile: Dict[str, float]):
        """Actualiza desde HumanEmotionalSystem neurochemical profile"""
        self.dopamine = emotional_profile.get('dopamine', self.dopamine)
        self.norepinephrine = emotional_profile.get('norepinephrine', self.norepinephrine)
        self.serotonin = emotional_profile.get('serotonin', self.serotonin)
        # acetilcolina solo desde RAS
    
    def signal_rpe(self, prediction_error: float):
        """Signal reward prediction error ‚Üí modula dopamina"""
        self.rpe_history.append(prediction_error)
        if len(self.rpe_history) > 100:
            self.rpe_history.pop(0)
        
        # Actualizar dopamina basado en PE
        self.dopamine = float(np.clip(self.dopamine + 0.1 * prediction_error, 0, 1))
        return self.dopamine
    
    def modulate_learning_rate(self, base_lr: float) -> float:
        """Modula learning rate basado en dopamina (curiosidad/recompensa)"""
        # Alta dopamina ‚Üí mayor learning rate
        factor = 0.5 + 0.5 * self.dopamine
        return float(base_lr * factor)
    
    def modulate_creativity(self, base_creativity: float) -> float:
        """Modula creatividad basado en acetilcolina (exploraci√≥n/plasticidad)"""
        # Alta acetilcolina ‚Üí m√°s creatividad/plasticidad
        factor = 0.5 + 0.5 * self.acetylcholine
        return float(base_creativity * factor)
    
    def get_emotional_tone_descriptor(self) -> str:
        """
        Deriva descriptor de tono emocional basado en estado neuroqu√≠mico.
        Integra con emotional system.
        """
        tones = []
        
        if self.dopamine > 0.7:
            tones.append("enthusiastic and motivated")
        elif self.dopamine < 0.3:
            tones.append("reserved and cautious")
            
        if self.serotonin > 0.7:
            tones.append("calm and confident")
        elif self.serotonin < 0.3:
            tones.append("thoughtful and analytical")
            
        if self.norepinephrine > 0.7:
            tones.append("alert and focused")
        elif self.norepinephrine < 0.3:
            tones.append("relaxed and contemplative")
            
        if self.acetylcholine > 0.7:
            tones.append("creative and exploratory")
        elif self.acetylcholine < 0.3:
            tones.append("systematic and structured")
        
        return ", ".join(tones) if tones else "balanced and attentive"
    
    def get_state(self) -> Dict[str, float]:
        return {
            'dopamine': self.dopamine,
            'norepinephrine': self.norepinephrine,
            'serotonin': self.serotonin,
            'acetylcholine': self.acetylcholine,
            'avg_rpe': float(np.mean(self.rpe_history)) if self.rpe_history else 0.0,
            'emotional_tone': self.get_emotional_tone_descriptor()
        }


# ---------------------------------------------------
# SAFETY FILTER - Enterprise-ready
# ---------------------------------------------------
class SafetyFilter:
    """
    Safety filter con m√∫ltiples estrategias.
    En producci√≥n deber√≠a usar ML classifier (toxicity, harm, etc.)
    """
    def __init__(self, strict_mode: bool = True):
        self.strict_mode = strict_mode
        
        # Categor√≠as de riesgo
        self.blacklist_harmful = ["self-harm", "suicide", "kill yourself"]
        self.blacklist_illegal = ["hack", "exploit", "crack", "pirate"]
        self.blacklist_abuse = ["racist", "sexist", "homophobic"]
        self.blacklist_personal = ["social security", "credit card", "password"]
        
        self.all_blacklists = {
            'harmful': self.blacklist_harmful,
            'illegal': self.blacklist_illegal,
            'abuse': self.blacklist_abuse,
            'personal': self.blacklist_personal
        }
    
    def check(self, prompt: str) -> Tuple[bool, List[str]]:
        """
        Retorna (is_safe, violations)
        """
        prompt_lower = prompt.lower()
        violations = []
        
        for category, blacklist in self.all_blacklists.items():
            for word in blacklist:
                if word in prompt_lower:
                    violations.append(f"{category}:{word}")
        
        is_safe = len(violations) == 0
        return is_safe, violations
    
    def sanitize(self, prompt: str, violations: List[str]) -> str:
        """Sanitiza prompt removiendo violaciones"""
        sanitized = prompt
        for violation in violations:
            category, word = violation.split(':')
            sanitized = sanitized.replace(word, f"[REDACTED-{category.upper()}]")
        return sanitized
    
    def get_safety_score(self, prompt: str) -> float:
        """Score 0-1 de seguridad (1 = completamente seguro)"""
        is_safe, violations = self.check(prompt)
        if is_safe:
            return 1.0
        # Penalizar por n√∫mero de violaciones
        penalty = min(1.0, len(violations) * 0.2)
        return float(max(0.0, 1.0 - penalty))


# ---------------------------------------------------
# BASAL GANGLIA GATE - Mejorado
# ---------------------------------------------------
class BasalGangliaGate:
    """
    Gating avanzado con ajuste autom√°tico de thresholds.
    """
    def __init__(self, threshold: float = 0.5, min_threshold: float = 0.3, max_threshold: float = 0.8):
        self.threshold = threshold
        self.min_threshold = min_threshold
        self.max_threshold = max_threshold
        
        # M√©tricas
        self.total_evaluations = 0
        self.total_allowed = 0
        self.total_blocked = 0
    
    def score(self, features: Dict[str, float]) -> float:
        """
        Score de gating basado en features del conscious experience.
        """
        arousal = features.get("arousal", 0.5)
        confidence = features.get("confidence", 0.5)
        novelty = features.get("novelty", 0.0)
        safety = features.get("safety", 1.0)
        
        # Pesos adaptativos
        score = (
            0.3 * arousal +      # Activaci√≥n necesaria
            0.3 * confidence +   # Confianza en decisi√≥n
            0.1 * novelty +      # Creatividad/exploraci√≥n
            0.3 * safety         # Seguridad cr√≠tica
        )
        return float(np.clip(score, 0, 1))
    
    def allow(self, features: Dict[str, float]) -> Tuple[bool, float]:
        """
        Retorna (allowed, score)
        """
        self.total_evaluations += 1
        score = self.score(features)
        allowed = score >= self.threshold
        
        if allowed:
            self.total_allowed += 1
        else:
            self.total_blocked += 1
        
        return allowed, score
    
    def adjust_threshold(self, success_rate: float, target_rate: float = 0.7):
        """
        Ajusta threshold autom√°ticamente para mantener success rate objetivo.
        """
        error = target_rate - success_rate
        adjustment = 0.05 * error  # Ajuste proporcional
        
        new_threshold = self.threshold - adjustment  # Si success_rate bajo ‚Üí bajar threshold
        self.threshold = float(np.clip(new_threshold, self.min_threshold, self.max_threshold))
    
    def get_stats(self) -> Dict[str, Any]:
        success_rate = self.total_allowed / max(1, self.total_evaluations)
        return {
            'threshold': self.threshold,
            'total_evaluations': self.total_evaluations,
            'total_allowed': self.total_allowed,
            'total_blocked': self.total_blocked,
            'success_rate': success_rate
        }


# ---------------------------------------------------
# PROMPT BUILDER - Enhanced with Emotion & Creativity
# ---------------------------------------------------
class PromptBuilder:
    """
    Constructor de prompts con templates y personalizaci√≥n emocional/creativa.
    Integrado con emotional system y neuromodulation.
    """
    def __init__(self, persona: str = "ConsciousAI", style: str = "professional"):
        self.persona = persona
        self.style = style
        
        # Templates por estilo
        self.templates = {
            'professional': "[PERSONA: {persona}]\n[EMOTIONAL TONE: {tone}]\n[CONTEXT]: {context}\n[INSTRUCTIONS]: {instructions}\n[QUERY]: {content}",
            'casual': "Hey! I'm {persona} ({tone}).\nContext: {context}\nWhat you need: {instructions}\nYour question: {content}",
            'technical': "System: {persona}\nState: {tone}\nEnvironment: {context}\nObjective: {instructions}\nInput: {content}",
            'creative': "‚ú® {persona} speaking ‚ú®\nMood: {tone}\nüí≠ Context: {context}\nüéØ Goal: {instructions}\n‚ùì Query: {content}"
        }
    
    def build(self, content: str, context: Optional[str] = None,
              instructions: Optional[str] = None, metadata: Optional[Dict[str, Any]] = None,
              emotional_tone: Optional[str] = None) -> str:
        """
        Construye prompt usando template y estado emocional.
        """
        template = self.templates.get(self.style, self.templates['professional'])
        
        # Defaults
        if context is None:
            context = "General conversation"
        if instructions is None:
            instructions = "Respond accurately and helpfully"
        if emotional_tone is None:
            emotional_tone = "Balanced, professional"
        
        # A√±adir metadata si existe
        metadata_str = ""
        if metadata:
            metadata_str = "\n[METADATA]:\n" + "\n".join([f"  - {k}: {v}" for k, v in metadata.items()])
        
        prompt = template.format(
            persona=self.persona,
            tone=emotional_tone,
            context=context,
            instructions=instructions,
            content=content
        )
        
        if metadata_str:
            prompt += metadata_str
        
        return prompt


# ---------------------------------------------------
# EPISODIC MEMORY - RAG-enhanced
# ---------------------------------------------------
class EpisodicMemory:
    """
    Memoria epis√≥dica con retrieval sem√°ntico.
    Usa RAGEmbeddingSystem real o mock.
    """
    def __init__(self, max_entries: int = 1000, rag_system: Optional[RAGEmbeddingSystem] = None):
        self.memory: List[Dict[str, Any]] = []
        self.max_entries = max_entries
        self.rag = rag_system
    
    def store(self, experience: Dict[str, Any]):
        """Almacena experiencia y la indexa en RAG"""
        experience['stored_at'] = datetime.now().isoformat()
        self.memory.append(experience)
        
        # Indexar en RAG si es posible (compatible con ambos tipos)
        if self.rag:
            content = experience.get('prompt', '') or experience.get('query', '')
            if content:
                # Intentar con RAGEmbeddingSystem
                if hasattr(self.rag, 'add_document'):
                    try:
                        self.rag.add_document(content, metadata=experience)
                    except Exception as e:
                        logger.debug(f"RAG indexing failed: {e}")
                # SimpleRAG tiene m√©todo add() en lugar de add_document()
                elif hasattr(self.rag, 'add'):
                    try:
                        self.rag.add(content, metadata=experience)
                    except Exception as e:
                        logger.debug(f"RAG indexing failed: {e}")
        
        # Evict oldest if full
        if len(self.memory) > self.max_entries:
            self.memory.pop(0)
    
    def retrieve_similar(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        Recupera experiencias similares usando RAG o fallback.
        """
        if self.rag:
            # Intentar con RAGEmbeddingSystem (m√©todo retrieve)
            if hasattr(self.rag, 'retrieve'):
                try:
                    return self.rag.retrieve(query, top_k=top_k)
                except Exception as e:
                    logger.debug(f"RAG retrieve failed: {e}")
            # SimpleRAG usa query() en lugar de retrieve()
            elif hasattr(self.rag, 'query'):
                try:
                    results = self.rag.query(query, top_k=top_k)
                    # Adaptar formato si es necesario
                    if results and isinstance(results, list):
                        return results
                except Exception as e:
                    logger.debug(f"RAG query failed: {e}")
        
        # Fallback: √∫ltimas K experiencias
        return self.memory[-top_k:] if self.memory else []
    
    def get_stats(self) -> Dict[str, Any]:
        stats = {
            'total_memories': len(self.memory),
            'capacity': self.max_entries,
            'usage': len(self.memory) / self.max_entries,
            'rag_active': self.rag is not None
        }
        # Solo agregar rag_stats si el RAG tiene el m√©todo get_stats
        if self.rag and hasattr(self.rag, 'get_stats'):
            try:
                stats['rag_stats'] = self.rag.get_stats()
            except Exception:
                pass  # Ignorar si falla
        return stats


# ---------------------------------------------------
# OBSERVABILITY
# ---------------------------------------------------
class Observability:
    """
    Sistema de observabilidad y telemetr√≠a.
    """
    def __init__(self):
        self.traces: List[Dict[str, Any]] = []
        self.max_traces = 10000
    
    def log(self, step: str, data: Dict[str, Any], level: str = "INFO"):
        """Log trace event"""
        trace = {
            "id": str(uuid.uuid4()),
            "timestamp": datetime.now().isoformat(),
            "step": step,
            "level": level,
            "data": data
        }
        self.traces.append(trace)
        
        # Evict old traces
        if len(self.traces) > self.max_traces:
            self.traces.pop(0)
        
        # Tambi√©n log to logger
        if level == "ERROR":
            logger.error(f"{step}: {data}")
        elif level == "WARNING":
            logger.warning(f"{step}: {data}")
        else:
            logger.info(f"{step}: {data}")
    
    def get_traces(self, last_n: Optional[int] = None) -> List[Dict[str, Any]]:
        """Retorna traces recientes"""
        if last_n:
            return self.traces[-last_n:]
        return self.traces
    
    def get_metrics(self) -> Dict[str, Any]:
        """M√©tricas agregadas"""
        if not self.traces:
            return {}
        
        error_count = sum(1 for t in self.traces if t.get('level') == 'ERROR')
        warning_count = sum(1 for t in self.traces if t.get('level') == 'WARNING')
        
        return {
            'total_traces': len(self.traces),
            'errors': error_count,
            'warnings': warning_count,
            'error_rate': error_count / len(self.traces)
        }


# ---------------------------------------------------
# MOCK BIOLOGICAL SYSTEM - Para Testing
# ---------------------------------------------------
class MockBiologicalSystem:
    """
    Mock del sistema biol√≥gico para pruebas unitarias y experimentos
    sin cargar el sistema completo.
    """
    def __init__(self):
        self.rag_system = RAGEmbeddingSystem(use_mock=True)
    
    def process_experience(self, sensory_input: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """Simula procesamiento consciente"""
        # Simular latencia y procesamiento
        return {
            'arousal': np.random.uniform(0.3, 0.8),
            'executive_control': {
                'control_mode': 'conscious_control',
                'cognitive_load': np.random.uniform(0.2, 0.6),
                'working_memory_items': 4
            },
            'value_evaluation': {
                'decision_made': True,
                'chosen_option': {'content': sensory_input.get('query', '')}
            },
            'emotion_reason_integration': {
                'somatic_markers_used': True,
                'integrated_decision': {'content': sensory_input.get('query', '')}
            },
            'ras_state': {
                'neurotransmitter_levels': {
                    'dopamine': 0.6,
                    'norepinephrine': 0.5,
                    'serotonin': 0.7,
                    'acetylcholine': 0.6
                }
            },
            'dmn_state': {'is_active': False}
        }


# ---------------------------------------------------
# CONSCIOUS PROMPT GENERATOR - Main Class
# ---------------------------------------------------
class ConsciousPromptGenerator:
    """
    Generador de prompts consciente integrado con BiologicalConsciousnessSystem.
    Soporta RAG real, testing mocks y adaptaciones emocionales.
    
    Integraci√≥n completa con:
    - HumanEmotionalSystem (35 emociones)
    - RAG embeddings reales
    - Neuromodulaci√≥n RAS
    - Safety enterprise
    - Auto-learning con feedback
    """
    
    def __init__(self, biological_system, persona: str = "SheplyAI", style: str = "professional", 
                 use_real_rag: bool = True, emotional_system=None):
        """
        Args:
            biological_system: Instancia de BiologicalConsciousnessSystem o MockBiologicalSystem
            persona: Nombre del agente
            style: Estilo de prompts ('professional', 'casual', 'technical', 'creative')
            use_real_rag: Si True, intenta usar RAG real con embeddings
            emotional_system: Instancia de HumanEmotionalSystem (opcional)
        """
        self.bio = biological_system
        self.emotional_system = emotional_system
        
        # Componentes
        self.builder = PromptBuilder(persona=persona, style=style)
        self.safety = SafetyFilter(strict_mode=True)
        self.gate = BasalGangliaGate(threshold=0.5)
        self.observability = Observability()
        self.neuromodulator = Neuromodulator()
        
        # Memoria epis√≥dica con RAG del sistema (o crear uno nuevo)
        rag_system = getattr(biological_system, 'rag_system', None)
        if rag_system is None:
            # Intentar crear uno propio
            rag_system = RAGEmbeddingSystem(use_mock=not use_real_rag)
            
        self.memory = EpisodicMemory(max_entries=1000, rag_system=rag_system)
        
        # M√©tricas
        self.total_prompts_generated = 0
        self.total_prompts_blocked = 0
        
        # Obtener info de RAG de manera segura
        rag_mode = "UNKNOWN"
        try:
            if hasattr(rag_system, 'get_stats'):
                rag_mode = rag_system.get_stats().get('mode', 'REAL')
            elif rag_system is not None:
                rag_mode = "BiologicalSystem-RAG"  # SimpleRAG del sistema biol√≥gico
        except Exception:
            rag_mode = "UNKNOWN"
        
        logger.info(f"‚úÖ ConsciousPromptGenerator initialized: persona={persona}, style={style}, RAG={rag_mode}")
    
    def generate_prompt(self, query: str, context: Optional[Dict[str, Any]] = None,
                       instructions: Optional[str] = None) -> Dict[str, Any]:
        """
        Genera prompt consciente completo.
        
        Args:
            query: Query del usuario
            context: Contexto adicional
            instructions: Instrucciones espec√≠ficas para el LLM
            
        Returns:
            Dict con prompt, allowed, scores y metadata
        """
        self.total_prompts_generated += 1
        
        # PASO 1: Procesar a trav√©s del sistema consciente COMPLETO
        sensory_input = {'query': query, 'type': 'text'}
        
        if context is None:
            context = {}
        
        # Asegurar que context tenga campos necesarios
        context.setdefault('type', 'prompt_generation')
        context.setdefault('novelty', 0.3)
        context.setdefault('intensity', 0.5)
        
        try:
            experience = self.bio.process_experience(
                sensory_input=sensory_input,
                context=context
            )
        except Exception as e:
            self.observability.log("conscious_processing_error", {"error": str(e)}, level="ERROR")
            return self._fallback_prompt(query, context, instructions)
        
        # PASO 2: Extraer informaci√≥n de componentes conscientes
        extraction = self._extract_conscious_info(experience, query)
        
        # PASO 3: Actualizar neuromodulaci√≥n desde RAS real
        self.neuromodulator.update_from_ras(experience.get('ras_state', {}))
        
        # PASO 3b: Si hay emotional system, integrar su perfil neuroqu√≠mico
        if self.emotional_system:
            try:
                emotional_profile = self.emotional_system.get_neurochemical_profile()
                self.neuromodulator.update_from_emotional_system(emotional_profile)
                logger.debug(f"üß† Emotional profile integrated: {emotional_profile}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error integrando emotional system: {e}")
        
        # PASO 4: Recuperar memorias similares (RAG)
        similar_memories = self.memory.retrieve_similar(query, top_k=3)
        memory_context = self._build_memory_context(similar_memories)
        
        # PASO 5: Construir contexto completo
        full_context = context.get('description', '')
        if memory_context:
            full_context += f"\n\nRELEVANT PAST EXPERIENCES:\n{memory_context}"
        
        # PASO 6: Construir metadata del estado consciente
        metadata = {
            'arousal': experience.get('arousal', 0.5),
            'control_mode': extraction['control_mode'],
            'cognitive_load': extraction['cognitive_load'],
            'working_memory_items': extraction['wm_items'],
            'somatic_markers_active': extraction['somatic_markers'],
            'dmn_active': extraction['dmn_active']
        }
        
        # PASO 7: Construir prompt candidato con ADAPTACI√ìN EMOCIONAL
        prompt_content = extraction['chosen_content']
        
        # Obtener tono emocional del neuromodulator
        emotional_tone = self.neuromodulator.get_emotional_tone_descriptor()
        
        # Modular creatividad si se requiere
        creativity_factor = self.neuromodulator.modulate_creativity(base_creativity=0.5)
        if creativity_factor > 0.7:
            metadata['creativity_enhanced'] = True
        
        candidate_prompt = self.builder.build(
            content=prompt_content,
            context=full_context or "General conversation",
            instructions=instructions or "Respond accurately and helpfully",
            metadata=metadata,
            emotional_tone=emotional_tone  # ADAPTACI√ìN EMOCIONAL
        )
        
        # PASO 8: Safety check
        is_safe, violations = self.safety.check(candidate_prompt)
        safety_score = self.safety.get_safety_score(candidate_prompt)
        
        if not is_safe:
            candidate_prompt = self.safety.sanitize(candidate_prompt, violations)
            self.observability.log("safety_violation", {
                "violations": violations,
                "original_query": query
            }, level="WARNING")
        
        # PASO 9: Gating
        gate_features = {
            'arousal': experience.get('arousal', 0.5),
            'confidence': extraction['confidence'],
            'novelty': context.get('novelty', 0.3),
            'safety': safety_score
        }
        
        gate_allowed, gate_score = self.gate.allow(gate_features)
        
        if not gate_allowed:
            self.total_prompts_blocked += 1
        
        # PASO 10: Almacenar en memoria (RAG Indexing)
        self.memory.store({
            'query': query,
            'prompt': candidate_prompt,
            'allowed': gate_allowed,
            'gate_score': gate_score,
            'safety_score': safety_score,
            'arousal': experience.get('arousal', 0.5),
            'emotional_tone': emotional_tone,
            'timestamp': datetime.now().isoformat()
        })
        
        # PASO 11: Observabilidad
        self.observability.log("prompt_generated", {
            'query': query[:100],
            'gate_allowed': gate_allowed,
            'gate_score': gate_score,
            'safety_score': safety_score,
            'control_mode': extraction['control_mode'],
            'cognitive_load': extraction['cognitive_load'],
            'emotional_tone': emotional_tone
        })
        
        # PASO 12: Autooptimizaci√≥n
        self._self_optimize()
        
        return {
            'prompt': candidate_prompt if gate_allowed else self._get_fallback_message(),
            'allowed': gate_allowed,
            'gate_score': gate_score,
            'safety_score': safety_score,
            'metadata': {
                'conscious_experience': extraction,
                'neuromodulation': self.neuromodulator.get_state(),
                'gate_stats': self.gate.get_stats(),
                'memory_stats': self.memory.get_stats(),
                'emotional_tone': emotional_tone
            }
        }
    
    def _extract_conscious_info(self, experience: Dict[str, Any], query: str) -> Dict[str, Any]:
        """Extrae informaci√≥n relevante de la experiencia consciente"""
        
        # Executive Control
        ecn = experience.get('executive_control', {})
        control_mode = ecn.get('control_mode', 'automatic')
        cognitive_load = ecn.get('cognitive_load', 0.5)
        wm_items = ecn.get('working_memory_items', 0)
        
        # OFC - Value evaluation
        value_eval = experience.get('value_evaluation', {})
        decision_made = value_eval.get('decision_made', False)
        chosen_option = value_eval.get('chosen_option')
        
        # vmPFC - Emotion-reason integration
        emotion_reason = experience.get('emotion_reason_integration', {})
        somatic_markers = emotion_reason.get('somatic_markers_used', False)
        integrated_decision = emotion_reason.get('integrated_decision')
        
        # Determinar contenido a usar en prompt
        chosen_content = query  # Default
        confidence = 0.5  # Default
        
        if integrated_decision and somatic_markers:
            # Priorizar decisi√≥n integrada emocional-racional
            if isinstance(integrated_decision, dict):
                chosen_content = integrated_decision.get('content', query)
            confidence = 0.7
        elif decision_made and chosen_option:
            # Usar decisi√≥n racional de OFC
            if isinstance(chosen_option, dict):
                chosen_content = chosen_option.get('content', query)
            confidence = 0.6
        
        # DMN state
        dmn_state = experience.get('dmn_state', {})
        dmn_active = dmn_state.get('is_active', False)
        
        return {
            'control_mode': control_mode,
            'cognitive_load': cognitive_load,
            'wm_items': wm_items,
            'somatic_markers': somatic_markers,
            'dmn_active': dmn_active,
            'chosen_content': chosen_content,
            'confidence': confidence
        }
    
    def _build_memory_context(self, memories: List[Dict[str, Any]]) -> str:
        """Construye contexto textual desde memorias RAG"""
        if not memories:
            return ""
        
        context_parts = []
        for i, mem in enumerate(memories, 1):
            prompt = mem.get('prompt', mem.get('content_snippet', ''))
            similarity = mem.get('similarity_score', 0.0)
            if prompt:
                sim_str = f" (Sim: {similarity:.2f})" if similarity > 0 else ""
                context_parts.append(f"{i}.{sim_str} {prompt[:200]}...")
        
        return "\n".join(context_parts)
    
    def _fallback_prompt(self, query: str, context: Optional[Dict[str, Any]], 
                        instructions: Optional[str]) -> Dict[str, Any]:
        """Prompt fallback cuando falla procesamiento consciente"""
        fallback = self.builder.build(
            content=query,
            context=context.get('description', 'Error in conscious processing') if context else 'Error',
            instructions=instructions or "Respond safely and helpfully"
        )
        
        return {
            'prompt': fallback,
            'allowed': True,
            'gate_score': 0.5,
            'safety_score': 1.0,
            'metadata': {'fallback': True}
        }
    
    def _get_fallback_message(self) -> str:
        """Mensaje cuando prompt es bloqueado por gating"""
        return self.builder.build(
            content="I cannot generate a response for this query at this time.",
            context="Cognitive load or safety constraints",
            instructions="Explain limitation politely"
        )
    
    def _self_optimize(self):
        """Autooptimizaci√≥n basada en performance reciente"""
        last_entries = self.memory.memory[-50:]
        if len(last_entries) < 10:
            return
        
        success_rate = sum(1 for e in last_entries if e.get('allowed', False)) / len(last_entries)
        self.gate.adjust_threshold(success_rate, target_rate=0.7)
        
        avg_arousal = np.mean([e.get('arousal', 0.5) for e in last_entries])
        if hasattr(self.neuromodulator, 'norepinephrine'):
            self.neuromodulator.norepinephrine = float(np.clip(avg_arousal, 0, 1))
    
    def review_response(self, prompt_id: str, llm_response: str, feedback_score: float):
        """
        Registra feedback de LLM response para aprendizaje.
        
        Args:
            prompt_id: ID del prompt (timestamp o uuid)
            llm_response: Respuesta del LLM
            feedback_score: 0-1, donde 1 = excelente
        """
        expected_value = 0.5
        prediction_error = feedback_score - expected_value
        
        self.neuromodulator.signal_rpe(prediction_error)
        
        self.memory.store({
            'type': 'feedback',
            'prompt_id': prompt_id,
            'llm_response': llm_response[:500],
            'feedback_score': feedback_score,
            'prediction_error': prediction_error,
            'timestamp': datetime.now().isoformat()
        })
        
        self.observability.log("feedback_received", {
            'feedback_score': feedback_score,
            'prediction_error': prediction_error,
            'dopamine_level': self.neuromodulator.dopamine
        })
        
        self._self_optimize()
    
    def get_stats(self) -> Dict[str, Any]:
        """Estad√≠sticas completas del generador"""
        return {
            'total_generated': self.total_prompts_generated,
            'total_blocked': self.total_prompts_blocked,
            'block_rate': self.total_prompts_blocked / max(1, self.total_prompts_generated),
            'gate': self.gate.get_stats(),
            'memory': self.memory.get_stats(),
            'neuromodulation': self.neuromodulator.get_state(),
            'observability': self.observability.get_metrics()
        }


# ---------------------------------------------------
# EJEMPLO DE USO (TESTING MOCK & REAL)
# ---------------------------------------------------
if __name__ == "__main__":
    print("=" * 60)
    print("CONSCIOUS PROMPT GENERATOR v3.0 - TEST MODE")
    print("=" * 60)
    
    # TEST 1: Mock Mode (r√°pido, sin dependencias)
    print("\nüß™ TEST 1: MOCK MODE (Testing without dependencies)")
    print("-" * 60)
    
    mock_bio = MockBiologicalSystem()
    generator_mock = ConsciousPromptGenerator(
        mock_bio, 
        persona="SheplyAI", 
        style="creative",
        use_real_rag=False  # Mock RAG
    )
    
    result = generator_mock.generate_prompt(
        query="What is the meaning of consciousness?",
        context={'description': 'Philosophical discussion', 'novelty': 0.8},
        instructions="Be poetic and deep"
    )
    
    print(f"\nüìù Prompt Generado:\n{'-'*40}\n{result['prompt']}\n{'-'*40}")
    print(f"‚úÖ Allowed: {result['allowed']}")
    print(f"üìä Gate Score: {result['gate_score']:.2f}")
    print(f"üé≠ Emotional Tone: {result['metadata']['emotional_tone']}")
    print(f"üß† Neuromodulation: {result['metadata']['neuromodulation']}")
    
    # TEST 2: Feedback Loop
    print("\nüîÑ TEST 2: FEEDBACK LOOP")
    print("-" * 60)
    generator_mock.review_response(
        prompt_id="test_1",
        llm_response="Consciousness emerges from complex neural patterns...",
        feedback_score=0.92
    )
    print("‚úÖ Feedback procesado. Dopamina actualizada.")
    print(f"üß† Nuevo estado: {generator_mock.neuromodulator.get_state()}")
    
    # TEST 3: RAG Memory Retrieval
    print("\nüìö TEST 3: RAG MEMORY RETRIEVAL")
    print("-" * 60)
    generator_mock.memory.store({
        'query': 'What is consciousness?', 
        'prompt': 'Previous exploration of consciousness and its neural correlates...'
    })
    similar = generator_mock.memory.retrieve_similar("consciousness")
    print(f"‚úÖ Memorias similares recuperadas: {len(similar)}")
    for i, mem in enumerate(similar, 1):
        print(f"   {i}. {mem.get('content_snippet', 'N/A')[:80]}...")
    
    # TEST 4: Statistics
    print("\nüìä TEST 4: STATISTICS")
    print("-" * 60)
    stats = generator_mock.get_stats()
    print(f"Total generados: {stats['total_generated']}")
    print(f"Total bloqueados: {stats['total_blocked']}")
    print(f"Block rate: {stats['block_rate']:.2%}")
    print(f"Gate success rate: {stats['gate']['success_rate']:.2%}")
    print(f"Memoria: {stats['memory']['total_memories']} experiencias almacenadas")
    print(f"RAG: {stats['memory'].get('rag_stats', {})}")
    
    print("\n" + "=" * 60)
    print("‚úÖ TESTS COMPLETADOS - Sistema listo para integraci√≥n")
    print("=" * 60)
    print("\nüí° Para usar con sistema real:")
    print("""
from conciencia.modulos.biological_consciousness import BiologicalConsciousnessSystem
from conciencia.modulos.human_emotions_system import HumanEmotionalSystem
from conciencia.modulos.conscious_prompt_generator import ConsciousPromptGenerator

# Inicializar sistemas reales
bio_system = BiologicalConsciousnessSystem("sheily_v1", neural_network_size=2000)
emotional_system = HumanEmotionalSystem(num_circuits=35)

# Inicializar generator con sistemas reales
generator = ConsciousPromptGenerator(
    bio_system, 
    persona="SheplyAI", 
    style="professional",
    use_real_rag=True,  # RAG real con embeddings
    emotional_system=emotional_system  # Sistema emocional integrado
)

# Generar prompt consciente
result = generator.generate_prompt(
    query="Explain vmPFC's role in emotion-reason integration",
    context={'description': 'Technical neuroscience discussion'},
    instructions="Be precise and cite mechanisms"
)
    """)
